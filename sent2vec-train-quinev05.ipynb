{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/joy/Thesis\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string\n",
    "import codecs\n",
    "import mmap\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from importlib import reload\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import smart_open\n",
    "import statistics\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import nltk\n",
    "import sent2vec\n",
    "import sklearn\n",
    "import itertools\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training data\n",
    "\n",
    "Each line is preprocessed for training the Sent2Vec model. Sent2Vec trains on untokenised sentence documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(fname_source, fname_des):\n",
    "    source = smart_open.open(fname_source, encoding=\"iso-8859-1\")\n",
    "    des = smart_open.open(fname_des, 'w')\n",
    "    for line in source.readlines():\n",
    "        line = line.strip()\n",
    "        line = str(line.lower())\n",
    "        line = nltk.word_tokenize(line)\n",
    "        line = [word for word in line if (word.isalpha() or word == '.' or word == ',')]\n",
    "        for item in line:\n",
    "        #print(item)\n",
    "            des.write(\"%s \" % item)\n",
    "        des.write(\"\\n\")\n",
    "    return open(fname_des).readlines()\n",
    "\n",
    "#preprocess_data(cwd + '/Data/quinev05_input_word.txt', 'training_data_word2.txt')\n",
    "#preprocess_data(cwd + '/Data/quinev05_input_lemma.txt', 'training_data_lemma.txt')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sent2Vec model on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains Sent2Vec models for each gridsearch combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #!/bin/bash\n",
    "\n",
    "# griddict = {\n",
    "#         'e': [100, 200, 300], #epochs\n",
    "#         'd': [64, 128, 256], #vec dim\n",
    "#         'lr': [0.15, 0.2, 0.25, 0.3], #learning rate\n",
    "#         'minCount': [0, 5],\n",
    "#         'loss' : ['ns'],\n",
    "#         'drop': [2],\n",
    "#         'sub' : [1.0],\n",
    "#      }\n",
    "\n",
    "# keys, values = zip(*griddict.items())\n",
    "# calls = []\n",
    "# i = 0\n",
    "# total = str(len(list(itertools.product(*values))))\n",
    "# print(\"number of combinations: \" + total)\n",
    "# for v in itertools.product(*values):\n",
    "#     experiment = dict(zip(keys, v))\n",
    "#     # create filname from griddict values\n",
    "#     outfilename = '/media/joy/Seagate_usb/Models/ns-word/s2vquine05_tune_d' + str(experiment['d']) + '_e' + str(experiment['e']) + '_lr' + str(experiment['lr']) + '_min' + str(experiment['minCount']) + '_loss_' + str(experiment['loss']) + '_drop_' + str(experiment['drop']) + '_sub' + str(experiment['sub'])\n",
    "#     modelfilename = outfilename #+ '.bin'\n",
    "\n",
    "#     # CHANGE TO CORRECT TRAINING DATA FILE!\n",
    "#     command =\"! ./fasttext sent2vec -input training_data_word.txt -output \" + modelfilename + \" -dim \" + str(experiment['d']) + \" -epoch \" + str(experiment['e']) + \" -lr \" + str(experiment['lr']) + \" -minCount \" + str(experiment['minCount']) + \" -loss \" + str(experiment['loss']) + \" -dropoutK \" + str(experiment['drop']) + \" -t \" + str(experiment['sub'])\n",
    "#     print(i, \"/\", total, command)\n",
    "#     os.system(command)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tests2vqueryrun(train_file, test_annot, modelfile, query, n, docid):\n",
    "#This function can run the same evaluation multiple times, which is necessary for inferred query vectors, as they can be quite different each time. When using additive or trained query vectors, it just runs the evaluation once and the aggregate statistics don't mean anything.\n",
    "\n",
    "    \n",
    "    #loading the model\n",
    "    model = sent2vec.Sent2vecModel()\n",
    "    model.load_model(modelfile)\n",
    "    \n",
    "    # CHANGE PATH HERE\n",
    "    no_path = modelfile.replace('/media/joy/Seagate_usb/Models/Sent2Vec/best-word', '')\n",
    "    # Send log output to txt file\n",
    "    # CHANGE PATH HERE\n",
    "    txtfile = '/media/joy/Seagate_usb/Logs/Sent2Vec/ns/Test/word/paragraphs/9097' + no_path.replace(\".bin\", \".txt\")\n",
    "    logging.shutdown()\n",
    "    reload(logging)\n",
    "    logging.basicConfig(filename=txtfile,\n",
    "                    format='%(asctime)s %(message)s',\n",
    "                    filemode='w')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Preprocess training data file.\n",
    "    test_corpus = preprocess_data(cwd + '/Data/' + train_file, 'training_data.txt')\n",
    "\n",
    "    a = codecs.open(test_annot, encoding=\"utf-8\")\n",
    "    annot_corpus = a.readlines()\n",
    "    annot_corpus = [i.split() for i in annot_corpus]\n",
    "\n",
    "    apscore10_eval = []\n",
    "    apscore100_eval = []\n",
    "    apscore500_eval = []\n",
    "    f10_eval = []\n",
    "    f100_eval = []\n",
    "    f500_eval = []\n",
    "    pc10_eval = []\n",
    "    pc100_eval = []\n",
    "    pc500_eval = []\n",
    "    pcr_eval = []\n",
    "    pcr_r_eval = []\n",
    "    pcr_m_eval = []\n",
    "    pcr_i_eval = []\n",
    "    avgrank_all_eval = []\n",
    "    avgrank_500_eval = []\n",
    "    avgdist_all_eval = []\n",
    "    avgdist_500_eval = []\n",
    "\n",
    "    for i in range(n):\n",
    "        apscore10, apscore100, apscore500, pc10, pc100, pc500, f10, f100, f500, pcr, pcr_r, pcr_m, pcr_i, avgrank_all, avgrank_500, avgdist_all, avgdist_500 = tests2vquery(test_corpus, annot_corpus, model, query, logger, docid)\n",
    "        #print(apscore10, apscore100, apscore500, pc10, pc100, pc500, f10, f100, f500, pcr, pcr_r, pcr_m, pcr_i, avgrank_all, avgrank_500, avgdist_all, avgdist_500)\n",
    "        apscore10_eval.append(apscore10)\n",
    "        apscore100_eval.append(apscore100)\n",
    "        apscore500_eval.append(apscore500)\n",
    "        f10_eval.append(f10)\n",
    "        f100_eval.append(f100)\n",
    "        f500_eval.append(f500)\n",
    "        pc10_eval.append(pc10)\n",
    "        pc100_eval.append(pc100)\n",
    "        pc500_eval.append(pc500)\n",
    "        pcr_eval.append(pcr)\n",
    "        pcr_r_eval.append(pcr_r)\n",
    "        pcr_m_eval.append(pcr_m)\n",
    "        pcr_i_eval.append(pcr_i)\n",
    "        avgrank_all_eval.append(avgrank_all)\n",
    "        avgrank_500_eval.append(avgrank_500)\n",
    "        avgdist_all_eval.append(avgdist_all)\n",
    "        avgdist_500_eval.append(avgdist_500)\n",
    "\n",
    "    print(pcr)\n",
    "    logger.info('Over {} query runs:'.format(n))\n",
    "    logger.info('Mean Precision @ 10: {}, Mean Precision @ 100: {}, Mean Precision @ 500: {}'.format(sum(pc10_eval)/float(len(pc10_eval)), sum(pc100_eval)/float(len(pc100_eval)), sum(pc500_eval)/float(len(pc500_eval))))\n",
    "    logger.info('Mean F-score @ 10: {}, Mean F-score @ 100: {}, Mean F-score @ 500: {}'.format(sum(f10_eval)/float(len(f10_eval)), sum(f100_eval)/float(len(f100_eval)), sum(f500_eval)/float(len(f500_eval))))\n",
    "    logger.info('Mean Average Precision @ 10: {}, Mean Average Precision @ 100: {}, Mean Average Precision @ 500: {}'.format(sum(apscore10_eval)/float(len(apscore10_eval)), sum(apscore100_eval)/float(len(apscore100_eval)), sum(apscore500_eval)/float(len(apscore500_eval))))\n",
    "    logger.info('Mean Precision @ R: {}'.format(sum(pcr_eval)/float(len(pcr_eval))))\n",
    "    logger.info('Mean Precision @ R for Relevant results: {}'.format(sum(pcr_r_eval)/float(len(pcr_r_eval))))\n",
    "    logger.info('Mean Precision @ R for Mildly Relevant results: {}'.format(sum(pcr_m_eval)/float(len(pcr_m_eval))))\n",
    "    logger.info('Mean Precision @ R for Irrelevant results: {}'.format(sum(pcr_i_eval)/float(len(pcr_i_eval))))\n",
    "    logger.info('Average rank of targets: {}'.format(sum(avgrank_all_eval)/float(len(avgrank_all_eval))))\n",
    "    logger.info('Average rank of top 500 targets: {}'.format(sum(avgrank_500_eval)/float(len(avgrank_500_eval))))\n",
    "    logger.info('Average distance of target to query: {}'.format(sum(avgdist_all_eval)/float(len(avgdist_all_eval))))\n",
    "    logger.info('Average distance of top 500 targets to query: {}'.format(sum(avgdist_500_eval)/float(len(avgdist_500_eval))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(emb_matrix, qvector, n):\n",
    "    top_n = []\n",
    "    matrix = sklearn.metrics.pairwise.cosine_similarity(emb_matrix, qvector, dense_output=True)\n",
    "    indices = np.argpartition(matrix, -n, axis = 0)[-n:].flatten()\n",
    "    scores = np.partition(matrix, -n, axis = 0)[-n:].flatten()\n",
    "    for index, score in zip(indices, scores):\n",
    "        top_n.append((index, score))\n",
    "    return sorted(top_n, reverse = True,  key=lambda x: x[1])\n",
    "\n",
    "def tests2vquery(test_corpus, annot_corpus, model, query, logger, docid):\n",
    "    #Performs a single query and computes evaluation metrics over it. Supports summed query vectors (add), inferred query vectors (inf) and taking a trained document vector as a query vector (trn).\n",
    "    #If a docID is provided, it will use that as a query, otherwise it will use 'query'.\n",
    "    \n",
    "    annot_hits = []\n",
    "    annot_hits_m = []\n",
    "    annot_hits_r = []\n",
    "    annot_hits_i = []\n",
    "    for i, line in enumerate(annot_corpus):\n",
    "        if line:\n",
    "            if '@' in line[0]:\n",
    "                if '@1' in line[0]:\n",
    "                    annot_hits.append(i)\n",
    "                    annot_hits_r.append(i)\n",
    "                if '@0' in line[0]:\n",
    "                    annot_hits.append(i)\n",
    "                    annot_hits_m.append(i)\n",
    "                if '@-1' in line[0]:\n",
    "                    annot_hits_i.append(i)\n",
    "    #print('annot_hits', annot_hits)\n",
    "    if docid:\n",
    "        qvector = model.embed_sentence(test_corpus[docid])\n",
    "\n",
    "    else:\n",
    "        #qtokens = gensim.utils.simple_preprocess(query) \n",
    "        qvector = model.embed_sentence(query)\n",
    "\n",
    "    #logger.info('10 most similar words to query vector = {}'.format(model.similar_by_vector(qvector, topn=10)))\n",
    "\n",
    "    emb_matrix = model.embed_sentences(test_corpus)\n",
    "    sims = most_similar(emb_matrix, qvector, len(test_corpus))\n",
    "    sims10 = sims[:10]\n",
    "    sims100 = sims[:100]\n",
    "    \n",
    "    matches = {}\n",
    "    matches_r = {}\n",
    "    matches_m = {}\n",
    "    matches_i = {}\n",
    "    total = 0\n",
    "    top10 = 0\n",
    "    top1 = 0\n",
    "    top100 = 0\n",
    "    top500 = 0\n",
    "    top_r = 0 #For computing precision @ R (recall, the number of target documents)\n",
    "    eval_avgrank = []\n",
    "    eval_avg_distance_between = []\n",
    "    eval_avgrank_all = []\n",
    "    eval_avg_distance_between_all = []\n",
    "\n",
    "    total_r = 0\n",
    "    top10_r = 0\n",
    "    top1_r = 0\n",
    "    top100_r = 0\n",
    "    top500_r = 0\n",
    "    top_r_r = 0\n",
    "    eval_avgrank_r = []\n",
    "    eval_avg_distance_between_r = []\n",
    "    eval_avgrank_all_r = []\n",
    "    eval_avg_distance_between_all_r = []\n",
    "\n",
    "    total_m = 0\n",
    "    top10_m = 0\n",
    "    top1_m = 0\n",
    "    top100_m = 0\n",
    "    top500_m = 0\n",
    "    top_r_m = 0\n",
    "    eval_avgrank_m = []\n",
    "    eval_avg_distance_between_m = []\n",
    "    eval_avgrank_all_m = []\n",
    "    eval_avg_distance_between_all_m = []\n",
    "\n",
    "    total_i = 0\n",
    "    top10_i = 0\n",
    "    top1_i = 0\n",
    "    top100_i = 0\n",
    "    top500_i = 0\n",
    "    top_r_i = 0\n",
    "    eval_avgrank_i = []\n",
    "    eval_avg_distance_between_i = []\n",
    "    eval_avgrank_all_i = []\n",
    "    eval_avg_distance_between_all_i = []\n",
    "\n",
    "    total_match = len(annot_hits)\n",
    "    total_match_r = len(annot_hits_r)\n",
    "    total_match_m = len(annot_hits_m)\n",
    "    total_match_i = len(annot_hits_i)\n",
    "    total = len(annot_corpus)\n",
    "\n",
    "    #for line in annot_hits:\n",
    "    #    annotation = line.split(',')\n",
    "    rank = 1\n",
    "    for match in sims:\n",
    "        if match[0] in annot_hits:\n",
    "            #Count total hits of relevant passages\n",
    "            matches[match[0]] = (rank,match[1])\n",
    "            eval_avgrank_all.append(rank)\n",
    "            eval_avg_distance_between_all.append(match[1])\n",
    "            if rank < 501:\n",
    "                eval_avgrank.append(rank)\n",
    "                eval_avg_distance_between.append(match[1])\n",
    "                top500 = top500 + 1\n",
    "                if rank < 101:\n",
    "                    top100 = top100 + 1\n",
    "                    if rank < 11:\n",
    "                        top10 = top10 + 1\n",
    "                        if rank == 1:\n",
    "                            top1 = top1 + 1\n",
    "            if rank < total_match:\n",
    "                top_r = top_r + 1\n",
    "\n",
    "        if match[0] in annot_hits_r:\n",
    "            matches_r[match[0]] = (rank,match[1])\n",
    "            eval_avgrank_all_r.append(rank)\n",
    "            eval_avg_distance_between_all_r.append(match[1])\n",
    "            if rank < 501:\n",
    "                eval_avgrank_r.append(rank)\n",
    "                eval_avg_distance_between_r.append(match[1])\n",
    "                top500_r = top500_r + 1\n",
    "                if rank < 101:\n",
    "                    top100_r = top100_r + 1\n",
    "                    if rank < 11:\n",
    "                        top10_r = top10_r + 1\n",
    "                        if rank == 1:\n",
    "                            top1_r = top1_r + 1\n",
    "            if rank < total_match_r:\n",
    "                top_r_r = top_r_r + 1\n",
    "\n",
    "        if match[0] in annot_hits_m:\n",
    "            matches_m[match[0]] = (rank,match[1])\n",
    "            eval_avgrank_all_m.append(rank)\n",
    "            eval_avg_distance_between_all_m.append(match[1])\n",
    "            if rank < 501:\n",
    "                eval_avgrank_m.append(rank)\n",
    "                eval_avg_distance_between_m.append(match[1])\n",
    "                top500_m = top500_m + 1\n",
    "                if rank < 101:\n",
    "                    top100_m = top100_m + 1\n",
    "                    if rank < 11:\n",
    "                        top10_m = top10_m + 1\n",
    "                        if rank == 1:\n",
    "                            top1_m = top1_m + 1\n",
    "            if rank < total_match_m:\n",
    "                top_r_m = top_r_m + 1\n",
    "        #rank = rank + 1\n",
    "\n",
    "        if match[0] in annot_hits_i:\n",
    "            matches_i[match[0]] = (rank,match[1])\n",
    "            eval_avgrank_all_i.append(rank)\n",
    "            eval_avg_distance_between_all_i.append(match[1])\n",
    "            if rank < 501:\n",
    "                eval_avgrank_i.append(rank)\n",
    "                eval_avg_distance_between_i.append(match[1])\n",
    "                top500_i = top500_i + 1\n",
    "                if rank < 101:\n",
    "                    top100_i = top100_i + 1\n",
    "                    if rank < 11:\n",
    "                        top10_i = top10_i + 1\n",
    "                        if rank == 1:\n",
    "                            top1_i = top1_i + 1\n",
    "            if rank < total_match_i:\n",
    "                top_r_i = top_r_i + 1\n",
    "        rank = rank + 1\n",
    "        \n",
    "    i = 0\n",
    "  \n",
    "    #top_r = top_r_r + top_r_m\n",
    "    print(top_r,top_r_r , top_r_m, top_r_i)\n",
    "    #Compute Average Precision metric for the total, scientistic and modest hits\n",
    "    ap = []\n",
    "    ap_r = []\n",
    "    ap_m = []\n",
    "    ap_i = []\n",
    "    for rank in sorted(eval_avgrank):\n",
    "        i=i+1\n",
    "        ap.append(i/rank)\n",
    "        if rank in eval_avgrank_r:\n",
    "            ap_r.append(i/rank)\n",
    "        if rank in eval_avgrank_m:\n",
    "            ap_m.append(i/rank)\n",
    "        if rank in eval_avgrank_i:\n",
    "            ap_i.append(i/rank)\n",
    "\n",
    "    if ap:\n",
    "        apscore = sum(ap)/float(len(ap))\n",
    "    else:\n",
    "        apscore = 0\n",
    "    if ap_r:\n",
    "        apscore_r = sum(ap_r)/float(len(ap_r))\n",
    "    else:\n",
    "        apscore_r = 0\n",
    "    if ap_m:\n",
    "        apscore_m = sum(ap_m)/float(len(ap_m))\n",
    "    else:\n",
    "        apscore_m = 0\n",
    "    if ap_i:\n",
    "        apscore_i = sum(ap_i)/float(len(ap_i))\n",
    "    else:\n",
    "        apscore_i = 0\n",
    "\n",
    "    i = 0\n",
    "    ap100 = []\n",
    "    ap100_r = []\n",
    "    ap100_m = []\n",
    "    ap100_i = []\n",
    "    for rank in sorted(eval_avgrank):\n",
    "        if rank < 101:\n",
    "            i=i+1\n",
    "            ap100.append(i/rank)\n",
    "            if rank in eval_avgrank_r:\n",
    "                ap100_r.append(i/rank)\n",
    "            if rank in eval_avgrank_m:\n",
    "                ap100_m.append(i/rank)\n",
    "            if rank in eval_avgrank_i:\n",
    "                ap100_i.append(i/rank)\n",
    "    if ap100:\n",
    "        apscore100 = sum(ap100)/float(len(ap100))\n",
    "    else:\n",
    "        apscore100 = 0\n",
    "    if ap100_r:\n",
    "        apscore100_r = sum(ap100_r)/float(len(ap100_r))\n",
    "    else:\n",
    "        apscore100_r = 0\n",
    "    if ap100_m:\n",
    "        apscore100_m = sum(ap100_m)/float(len(ap100_m))\n",
    "    else:\n",
    "        apscore100_m = 0\n",
    "    if ap100_i:\n",
    "        apscore100_i = sum(ap100_i)/float(len(ap100_i))\n",
    "    else:\n",
    "        apscore100_i = 0\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    ap10 = []\n",
    "    ap10_r = []\n",
    "    ap10_m = []\n",
    "    ap10_i = []\n",
    "    for rank in sorted(eval_avgrank):\n",
    "        if rank < 11:\n",
    "            i=i+1\n",
    "            ap10.append(i/rank)\n",
    "            if rank in eval_avgrank_r:\n",
    "                ap10_r.append(i/rank)\n",
    "            if rank in eval_avgrank_m:\n",
    "                ap10_m.append(i/rank)\n",
    "            if rank in eval_avgrank_i:\n",
    "                ap10_i.append(i/rank)\n",
    "    if ap10:\n",
    "        apscore10 = sum(ap10)/float(len(ap10))\n",
    "    else:\n",
    "        apscore10 = 0\n",
    "    if ap10_r:\n",
    "        apscore10_r = sum(ap10_r)/float(len(ap10_r))\n",
    "    else:\n",
    "        apscore10_r = 0\n",
    "    if ap10_m:\n",
    "        apscore10_m = sum(ap10_m)/float(len(ap10_m))\n",
    "    else:\n",
    "        apscore10_m = 0\n",
    "    if ap10_i:\n",
    "        apscore10_i = sum(ap10_i)/float(len(ap10_i))\n",
    "    else:\n",
    "        apscore10_i = 0\n",
    "\n",
    "    #Compute overall precision\n",
    "    pc10 = top10/float(10)\n",
    "    pc100 = top100/float(100)\n",
    "    pc500 = top500/float(500)\n",
    "    pcr = top_r/float(total_match) # R-Precision\n",
    "\n",
    "    #Compute overall recall\n",
    "    rc10 = top10/float(total_match)\n",
    "    rc100 = top100/float(total_match)\n",
    "    rc500 = top500/float(total_match)\n",
    "\n",
    "    #Compute specific precision\n",
    "    pcr_r = top_r_r/float(total_match_r)\n",
    "    pcr_m = top_r_m/float(total_match_m)\n",
    "    pcr_i = top_r_i/float(total_match_i)\n",
    "\n",
    "    #Compute F-scores\n",
    "    if pc10+rc10:\n",
    "        f10 = 2*(pc10*rc10)/(pc10+rc10)\n",
    "    else:\n",
    "        f10 = 0\n",
    "    if pc100+rc100:\n",
    "        f100 = 2*(pc100*rc100)/(pc100+rc100)\n",
    "    else:\n",
    "        f100 = 0\n",
    "    if pc500+rc500:\n",
    "        f500 = 2*(pc500*rc500)/(pc500+rc500)\n",
    "    else:\n",
    "        f500 = 0\n",
    "\n",
    "    logger.info('Target sentences were the top1 result {} times, were among the top 10 results {} times, among the top 100 results {} times, among the top 500 results {} times, out of {} target documents and {} documents'.format(top1, top10, top100, top500, total_match, total))\n",
    "    logger.info('Precision @ 10: {}, Precision @ 100: {}, Precision @ 500: {}, R-Precision: {}'.format(pc10, pc100, pc500, pcr))\n",
    "    logger.info('Recall @ 10: {}, Recall @ 100: {}, Recall @ 500: {}'.format(rc10, rc100, rc500))\n",
    "    logger.info('F-score @ 10: {}, F-score @ 100: {}, F-score @ 500: {}'.format(f10, f100, f500))\n",
    "    logger.info('Average Precision @ 10: {}, Average Precision @ 100: {}, Average Precision @ 500: {}'.format(apscore10, apscore100, apscore))\n",
    "\n",
    "    logger.info('In the top 10 are {} relevant, {} mildly relevant, {} irrelevant and {} unannotated results'.format(top10_r, top10_m, top10_i, 10-(top10_r + top10_m + top10_i)))\n",
    "    unannotated_top100 = 100-(top100_r + top100_m + top100_i)\n",
    "    unannotated_top500 = 500-(top500_r + top500_m + top500_i)\n",
    "    unannotated_top_r = total_match-(top_r_r + top_r_m + top_r_i)\n",
    "    logger.info('In the top 100 are {} relevant, {} mildly relevant, {} irrelevant and {} unannotated results'.format(top100_r, top100_m, top100_i, unannotated_top100))\n",
    "    logger.info('In the top 500 are {} relevant, {} mildly relevant, {} irrelevant and {} unannotated results'.format(top500_r, top500_m, top500_i, unannotated_top500))\n",
    "    logger.info('In the top R are {} relevant, {} mildly relevant, {} irrelevant and {} unannotated results, and R (for relevant and mildly relevant results) is {}'.format(top_r_r, top_r_m, top_r_i, unannotated_top_r, total_match))\n",
    "    logger.info('R-Precision is {} for relevant + mildly relevant results, {} for relevant results only, {} for mildly relevant results only and {} for irrelevant results only'.format(pcr, pcr_r, pcr_m, pcr_i))\n",
    "\n",
    "    if eval_avgrank:\n",
    "        if(len(eval_avgrank) == 1):\n",
    "            logger.info('Average rank of target sentences: {}. Average distance to query: {}. (range {}--{})'.format(sum(eval_avgrank)/float(len(eval_avgrank)), sum(eval_avg_distance_between)/float(len(eval_avg_distance_between)), min(eval_avg_distance_between), max(eval_avg_distance_between)))\n",
    "        #else:\n",
    "            #logger.info('Average rank of target sentences: {} (stdev {}). Average distance to query: {}. (Stdev {}, range {}--{})'.format(sum(eval_avgrank)/float(len(eval_avgrank)), statistics.stdev(eval_avgrank), sum(eval_avg_distance_between)/float(len(eval_avg_distance_between)), statistics.stdev(eval_avg_distance_between), min(eval_avg_distance_between), max(eval_avg_distance_between)))\n",
    "    else:\n",
    "        logger.info('No results')\n",
    "\n",
    "\n",
    "#top 10 matches\n",
    "    logger.info('Top 100 results for this query:')\n",
    "    rank = 1\n",
    "    for match in sims100:\n",
    "        logger.info('{} (similarity {}) - {} - {}'.format(rank, match[1], match[0], ' '.join(annot_corpus[match[0]])))\n",
    "        rank=rank+1\n",
    "#top10 targets\n",
    "    logger.info('The 10 highest ranked target results for this query:')\n",
    "    for key in sorted(matches, key=lambda x: matches[x][0]):\n",
    "        if matches[key][0] < 1001:\n",
    "            logger.info('{} (rank {}) - {} - {}'.format(key, matches[key][0], matches[key][1], annot_corpus[key]))\n",
    "\n",
    "#top10 targets\n",
    "    logger.info('The 10 highest results annotated as irrelevant for this query:')\n",
    "    for key in sorted(matches_i, key=lambda x: matches_i[x][0]):\n",
    "        if matches_i[key][0] < 1001:\n",
    "            logger.info('{} (rank {}) - {} - {}'.format(key, matches_i[key][0], matches_i[key][1], annot_corpus[key]))\n",
    "\n",
    "    if eval_avgrank:\n",
    "        #print(eval_avgrank)\n",
    "        return apscore10, apscore100, apscore, pc10, pc100, pc500, f10, f100, f500, pcr, pcr_r, pcr_m, pcr_i, sum(eval_avgrank_all)/float(len(eval_avgrank_all)), sum(eval_avgrank)/float(len(eval_avgrank)), sum(eval_avg_distance_between_all)/float(len(eval_avg_distance_between_all)), sum(eval_avg_distance_between)/float(len(eval_avg_distance_between))\n",
    "    else:\n",
    "        return apscore10, apscore100, apscore, pc10, pc100, pc500, f10, f100, f500, pcr, pcr_r, pcr_m, pcr_i, sum(eval_avgrank_all)/float(len(eval_avgrank_all)), 0, sum(eval_avg_distance_between_all)/float(len(eval_avg_distance_between_all)), 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests2vqueryrun('quinev05_input_word.txt', cwd + '/Data/quinev05_annotU_input_word.txt', 'model.bin', 'telepathy', 1, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2vquine05_tune_d128_e100_lr0.25_min0_loss_ns_drop_0.bin\n",
      "/media/joy/Seagate_usb/Models/Sent2Vec/best-word/s2vquine05_tune_d128_e100_lr0.25_min0_loss_ns_drop_0.bin\n",
      "16 15 0 0\n",
      "0.12307692307692308\n",
      "s2vquine05_tune_d128_e200_lr0.2_min5_loss_ns_drop_0.bin\n",
      "/media/joy/Seagate_usb/Models/Sent2Vec/best-word/s2vquine05_tune_d128_e200_lr0.2_min5_loss_ns_drop_0.bin\n",
      "13 10 0 0\n",
      "0.1\n",
      "s2vquine05_tune_d256_e100_lr0.25_min0_loss_ns.bin\n",
      "/media/joy/Seagate_usb/Models/Sent2Vec/best-word/s2vquine05_tune_d256_e100_lr0.25_min0_loss_ns.bin\n",
      "15 13 0 0\n",
      "0.11538461538461539\n",
      "s2vquine05_tune_d256_e100_lr0.25_min5_loss_ns.bin\n",
      "/media/joy/Seagate_usb/Models/Sent2Vec/best-word/s2vquine05_tune_d256_e100_lr0.25_min5_loss_ns.bin\n",
      "19 14 0 0\n",
      "0.14615384615384616\n",
      "s2vquine05_tune_d256_e100_lr0.3_min0_loss_ns.bin\n",
      "/media/joy/Seagate_usb/Models/Sent2Vec/best-word/s2vquine05_tune_d256_e100_lr0.3_min0_loss_ns.bin\n",
      "16 14 0 0\n",
      "0.12307692307692308\n",
      "s2vquine05_tune_d256_e200_lr0.2_min5_loss_ns_drop_0.bin\n",
      "/media/joy/Seagate_usb/Models/Sent2Vec/best-word/s2vquine05_tune_d256_e200_lr0.2_min5_loss_ns_drop_0.bin\n",
      "20 15 0 0\n",
      "0.15384615384615385\n",
      "s2vquine05_tune_d64_e200_lr0.3_min5_loss_ns.bin\n",
      "/media/joy/Seagate_usb/Models/Sent2Vec/best-word/s2vquine05_tune_d64_e200_lr0.3_min5_loss_ns.bin\n",
      "15 12 0 0\n",
      "0.11538461538461539\n"
     ]
    }
   ],
   "source": [
    "path = '/media/joy/Seagate_usb/Models/Sent2Vec/best-word/'\n",
    "for modelfilename in os.listdir(path):\n",
    "    print(modelfilename)\n",
    "    #print(path + modelfilename)\n",
    "    tests2vqueryrun('quinev05_input_word.txt', cwd + '/Data/quinev05_annotZ_rq1_input_word.txt', path + modelfilename, None, 1, 9097)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parselogs(indir, outfile):\n",
    "    with codecs.open(outfile, \"w\", encoding=\"utf-8\") as o:\n",
    "        csvhead = [\"filename\",\"p10\",\"p100\",\"p500\",\"f10\",\"f100\",\"f500\",\"map10\",\"map100\",\"map500\",\"mpr\",\"rank\",\"rank500\",\"dist\",\"dist500\"]\n",
    "        o.write('id,' + ','.join(csvhead) + '\\n')\n",
    "        run = 0\n",
    "        for filename in os.listdir(indir): \n",
    "            #if not re.match(\"d2vquine05test.word*.txt\", filename):\n",
    "                #continue\n",
    "            if os.stat(indir + '/' + filename).st_size == 0: #not empty files\n",
    "                continue\n",
    "            evaldict = {}\n",
    "            evaldict['filename'] = filename\n",
    "            res = 0\n",
    "            res = re.search('\\.v(\\d+\\.?\\d*)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['vectorsize'] = res.group(1)\n",
    "#             res = re.search('\\.e(\\d+\\.?\\d*)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['epochs'] = res.group(1)\n",
    "#             res = re.search('\\.w(\\d+\\.?\\d*)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['window'] = res.group(1)\n",
    "#             res = re.search('\\.s(\\d+\\.?\\d*)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['subsample'] = res.group(1)\n",
    "#             res = re.search('\\.c(\\d+\\.?\\d*)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['cutoff'] = res.group(1)\n",
    "#             res = re.search('\\.a(\\d+\\.?\\d*)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['alpha'] = res.group(1)\n",
    "#             res = re.search('\\.t(\\d+\\.?\\d*)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['bithreshold'] = res.group(1)\n",
    "#             res = re.search('\\.mc(\\d+\\.?\\d*)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['bicutoff'] = res.group(1)\n",
    "#             res = re.search('\\.d([a-z]+)', filename)\n",
    "#             if res:\n",
    "#                 evaldict['method'] = res.group(1)\n",
    "#             res = re.search('\\.bi', filename)\n",
    "#             if res:\n",
    "#                 evaldict['bigram'] = res.group(1)\n",
    "#             res = re.match('\\.hs', filename)\n",
    "#             if res:\n",
    "#                 evaldict['hs'] = 1\n",
    "#             else:\n",
    "#                 evaldict['hs'] = 0\n",
    "            res = re.match('_both', filename)\n",
    "#             if res:\n",
    "#                 evaldict['eval'] = 'both'\n",
    "#             else:\n",
    "#                 evaldict['eval'] = ''\n",
    "\n",
    "            with codecs.open(indir + '/' + filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                print(indir + '/' + filename)\n",
    "                #data = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)\n",
    "                data = f.readlines()\n",
    "                data = ' '.join(data)\n",
    "                print(len(data))\n",
    "                res = re.search('Mean Precision @ 10: (\\d+\\.?\\d*), Mean Precision @ 100: (\\d+\\.?\\d*), Mean Precision @ 500: (\\d+\\.?\\d*)', data)\n",
    "                if res:\n",
    "                    evaldict['p10'] = res.group(1)\n",
    "                    evaldict['p100'] = res.group(2)\n",
    "                    evaldict['p500'] = res.group(3)\n",
    "                \n",
    "                res = re.search('Mean F-score @ 10: (\\d+\\.?\\d*), Mean F-score @ 100: (\\d+\\.?\\d*), Mean F-score @ 500: (\\d+\\.?\\d*)', data)\n",
    "                if res:\n",
    "                    evaldict['f10'] = res.group(1)\n",
    "                    evaldict['f100'] = res.group(2)\n",
    "                    evaldict['f500'] = res.group(3)\n",
    "\n",
    "                res = re.search('Mean Average Precision @ 10: (\\d+\\.?\\d*), Mean Average Precision @ 100: (\\d+\\.?\\d*), Mean Average Precision @ 500: (\\d+\\.?\\d*)', data)\n",
    "                if res:\n",
    "                    evaldict['map10'] = res.group(1)\n",
    "                    evaldict['map100'] = res.group(2)\n",
    "                    evaldict['map500'] = res.group(3)\n",
    "\n",
    "                res = re.search('Mean Precision @ R: (\\d+\\.?\\d*)', data)\n",
    "                if res:\n",
    "                    evaldict['mpr'] = res.group(1)\n",
    "\n",
    "                res = re.search('Average rank of targets: (\\d+\\.?\\d*)', data)\n",
    "                if res:\n",
    "                    evaldict['rank'] = res.group(1)\n",
    "                    \n",
    "                res = re.search('Average rank of top 500 targets: (\\d+\\.?\\d*)', data)\n",
    "                if res:\n",
    "                    evaldict['rank500'] = res.group(1)\n",
    "\n",
    "                res = re.search('Average distance of target to query: (\\d+\\.?\\d*)', data)\n",
    "                if res:\n",
    "                    evaldict['dist'] = res.group(1)\n",
    "\n",
    "                res = re.search('Average distance of top 500 targets to query: (\\d+\\.?\\d*)', data)\n",
    "                if res:\n",
    "                    evaldict['dist500'] = res.group(1)\n",
    "\n",
    "#                 #additive vector\n",
    "#                 res = re.search('Testing model with additive query vector[\\s\\S]*Mean Precision @ 10: (\\d+\\.?\\d*), Mean Precision @ 100: (\\d+\\.?\\d*), Mean Precision @ 500: (\\d+\\.?\\d*)', data)\n",
    "#                 if res:\n",
    "#                     evaldict['p10add'] = res.group(1)\n",
    "#                     evaldict['p100add'] = res.group(2)\n",
    "#                     evaldict['p500add'] = res.group(3)\n",
    "                \n",
    "#                 res = re.search('Testing model with additive query vector[\\s\\S]*Mean F-score @ 10: (\\d+\\.?\\d*), Mean F-score @ 100: (\\d+\\.?\\d*), Mean F-score @ 500: (\\d+\\.?\\d*)', data)\n",
    "#                 if res:\n",
    "#                     evaldict['f10add'] = res.group(1)\n",
    "#                     evaldict['f100add'] = res.group(2)\n",
    "#                     evaldict['f500add'] = res.group(3)\n",
    "\n",
    "#                 res = re.search('Testing model with additive query vector[\\s\\S]*Mean Average Precision @ 10: (\\d+\\.?\\d*), Mean Average Precision @ 100: (\\d+\\.?\\d*), Mean Average Precision @ 500: (\\d+\\.?\\d*)', data)\n",
    "#                 if res:\n",
    "#                     evaldict['map10add'] = res.group(1)\n",
    "#                     evaldict['map100add'] = res.group(2)\n",
    "#                     evaldict['map500add'] = res.group(3)\n",
    "\n",
    "#                 res = re.search('Testing model with additive query vector[\\s\\S]*Mean Precision @ R: (\\d+\\.?\\d*)', data)\n",
    "#                 if res:\n",
    "#                     evaldict['mpradd'] = res.group(1)\n",
    "\n",
    "#                 res = re.search('Testing model with additive query vector[\\s\\S]*Average rank of targets: (\\d+\\.?\\d*)', data)\n",
    "#                 if res:\n",
    "#                     evaldict['rankadd'] = res.group(1)\n",
    "                    \n",
    "#                 res = re.search('Testing model with additive query vector[\\s\\S]*Average rank of top 500 targets: (\\d+\\.?\\d*)', data)\n",
    "#                 if res:\n",
    "#                     evaldict['rank500add'] = res.group(1)\n",
    "\n",
    "#                 res = re.search('Testing model with additive query vector[\\s\\S]*Average distance of target to query: (\\d+\\.?\\d*)', data)\n",
    "#                 if res:\n",
    "#                     evaldict['distadd'] = res.group(1)\n",
    "\n",
    "#                 res = re.search('Testing model with additive query vector[\\s\\S]*Average distance of top 500 targets to query: (\\d+\\.?\\d*)', data)\n",
    "#                 if res:\n",
    "#                     evaldict['dist500add'] = res.group(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #res = re.search('Part 1 term was the 1st nearest neighbour (\\d+\\.?\\d*) times', data)\n",
    "                #if res:\n",
    "                #    evaldict['top'] = res.group(1)\n",
    "                #res = re.search('and was among the top 10 nearest neighbours (\\d+\\.?\\d*) times', data)\n",
    "                #if res:\n",
    "                #    evaldict['top10'] = res.group(1)\n",
    "                #res = re.search('Part 1 baseline term was the 1st nearest neighbour of part 2 baseline (\\d+\\.?\\d*) times', data)\n",
    "                #if res:\n",
    "                #    evaldict['topbase'] = res.group(1)\n",
    "                #res = re.search('and was among the top 10 nearest neighbours of baseline (\\d+\\.?\\d*) times', data)\n",
    "                #if res:\n",
    "                #    evaldict['top10base'] = res.group(1)\n",
    "                #res = re.search(', out of (\\d+\\.?\\d*) terms', data)\n",
    "                #if res:\n",
    "                #    evaldict['total'] = res.group(1)\n",
    "            run = run + 1\n",
    "\n",
    "            #output file (csv)\n",
    "            o.write(str(run))\n",
    "            for column in csvhead:\n",
    "                if column in evaldict:\n",
    "                    o.write(','+ str(evaldict[column]))\n",
    "                else:\n",
    "                    o.write(',')\n",
    "            o.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parselogs('/media/joy/Seagate_usb/Logs/ns/Test/word/system_of_the_world', '/media/joy/Seagate_usb/Logs/ns/Test/word/system_of_the_world.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
